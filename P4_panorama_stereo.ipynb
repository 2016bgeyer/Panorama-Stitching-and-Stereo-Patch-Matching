{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:75% !important; margin-left: auto !important}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!jt -t monokai\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:75% !important; margin-left: auto !important}</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P4 Panoramas and Stereo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P4.1 Spherical Reprojection\n",
    "\n",
    "As we discussed in class, to make a panorama we need to reproject the images onto a sphere, something you will be implementing in this question. I have given you some starter code that you should use to reproject the image onto a sphere: the function `reproject_image_to_sphere`. I have annotated what you need to include to complete this function:\n",
    "\n",
    "<img src=\"annotated_projection_code.png\" width=\"600\">\n",
    "\n",
    "**TASK** Complete the `reproject_image_to_sphere` function I have provided below. I recommend that you revisit the lecture slides on panoramas to get the definitions of the unit sphere coordinates.\n",
    "\n",
    "I have provided you with a simple scene for Blender: `simple_pano_env.blend`. The camera is located at `x=0` and `y=0` and oriented such that it is level with the ground plane and rotated 0-degrees about the z-axis. The only camera in the scene has a Focal Length of 40 mm (expressed with respect to the *36 mm* film size standard used in photography). To test that your image reprojection method is working correctly.\n",
    "\n",
    "**TASK** Generate 4 images by changing the Focal Length of the camera in Blender and name them as follows:\n",
    "\n",
    "1. `b_pano_20mm.png` Rendered after setting the camera Focal Length to `20 mm`.\n",
    "2. `b_pano_30mm.png` Rendered after setting the camera Focal Length to `30 mm`.\n",
    "3. `b_pano_40mm.png` Rendered after setting the camera Focal Length to `40 mm`.\n",
    "4. `b_pano_50mm.png` Rendered after setting the camera Focal Length to `50 mm`.\n",
    "\n",
    "**Plots** Run the `Evaluation and Plotting` code I have included below. This will generate three figures (all of which you should include in your writeup). (1) shows the four images after the spherical reprojection. (2) shows the images added together, showing that in the center where all images have visibility of the scene, the images properly overlap. (3) The \"differences\" between consecutive Focal Lengths; if your code is implemented well, the center region (where the two overlap) should be nearly zero (\"white\" in the color scheme) and large outside of that image (where they do not overlap).\n",
    "\n",
    "If the second plot, in which all images have been added together, looks \"reasonable\" (that the images are properly overlapped with one another) and you are convinced that your reprojection function is working properly, you can move on to the next section, in which you are asked to build your own panoramas after reprojecting onto a sphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import scipy.interpolate\n",
    "\n",
    "# Helper Functions\n",
    "def load_image_gray(filepath):\n",
    "    \"\"\"Loads an image into a numpy array.\n",
    "    Note: image will have 3 color channels [r, g, b].\"\"\"\n",
    "    img = Image.open(filepath)\n",
    "    img = np.asarray(img).astype(np.float)/255\n",
    "    if len(img.shape) > 2:\n",
    "        return img[:, :, 0]\n",
    "    else:\n",
    "        return img\n",
    "\n",
    "def get_image_with_f(filepath, blender_focal_length_mm):\n",
    "    image = load_image_gray(filepath)\n",
    "    f = max(image.shape) * blender_focal_length_mm / 36.00\n",
    "    return image, f\n",
    "\n",
    "# Starter code for P4.1 (replace the None's in the loop)\n",
    "def reproject_image_to_sphere(image, focal_length_px, fov_deg=None, angular_resolution=0.01):\n",
    "    # Notice that because matrices are stored \"rows, columns\",\n",
    "    # we need to flip the \"shape\" coordinates so that the transformation\n",
    "    # matrix does what we expect. The other convention is also acceptable,\n",
    "    # as long as one is consistent. In this function, the transformation\n",
    "    # matrix is assumed to be in [x, y, w] coordinates, even though the image\n",
    "    # is stored in row, column (y, x) coordinates.\n",
    "    x = np.arange(image.shape[1]).astype(np.float)\n",
    "    y = np.arange(image.shape[0]).astype(np.float)\n",
    "    \n",
    "    # Compute the thetas and phis of the output plane\n",
    "    if fov_deg is None:\n",
    "        fov = np.arctan(max(image.shape)/focal_length_px/2) + angular_resolution\n",
    "    else:\n",
    "        fov = fov_deg * np.pi / 180\n",
    "    \n",
    "    print(f\"2 * Field of View: {2*fov}\")\n",
    "    thetas = np.arange(-fov, fov, angular_resolution)\n",
    "    phis = np.arange(-fov, fov, angular_resolution)\n",
    "\n",
    "    # Perform the interpolation\n",
    "    transformed_image = np.zeros((len(phis), len(thetas)))\n",
    "    image_fn = scipy.interpolate.interp2d(x, y, image, kind='linear', fill_value=0)\n",
    "    for ii in range(len(thetas)):\n",
    "        for jj in range(len(phis)):\n",
    "            theta = thetas[ii]\n",
    "            phi = phis[jj]\n",
    "            \n",
    "            xt = None\n",
    "            yt = None\n",
    "            zt = None\n",
    "            \n",
    "            new_x = len(x)//2 + None\n",
    "            new_y = len(y)//2 + None\n",
    "            transformed_image[jj, ii] = image_fn(new_x, new_y)\n",
    "    \n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "img_20, f_20 = get_image_with_f('b_pano_20mm.png', 20)\n",
    "img_30, f_30 = get_image_with_f('b_pano_30mm.png', 30)\n",
    "img_40, f_40 = get_image_with_f('b_pano_40mm.png', 40)\n",
    "img_50, f_50 = get_image_with_f('b_pano_50mm.png', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluation and Plotting Code\n",
    "\n",
    "sp_img_20 = reproject_image_to_sphere(img_20, f_20, fov_deg=45, angular_resolution=0.002)\n",
    "sp_img_30 = reproject_image_to_sphere(img_30, f_30, fov_deg=45, angular_resolution=0.002)\n",
    "sp_img_40 = reproject_image_to_sphere(img_40, f_40, fov_deg=45, angular_resolution=0.002)\n",
    "sp_img_50 = reproject_image_to_sphere(img_50, f_50, fov_deg=45, angular_resolution=0.002)\n",
    "\n",
    "plt.figure(figsize=(6,6), dpi=200)\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(sp_img_20)\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(sp_img_30)\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(sp_img_40)\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(sp_img_50)\n",
    "\n",
    "plt.figure(dpi=200)\n",
    "plt.imshow(sp_img_20 + sp_img_30 + sp_img_40 + sp_img_50)\n",
    "\n",
    "plt.figure(dpi=200)\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(sp_img_30 - sp_img_20, vmin=-0.2, vmax=0.2, cmap='PiYG')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(sp_img_40 - sp_img_30, vmin=-0.2, vmax=0.2, cmap='PiYG')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(sp_img_50 - sp_img_40, vmin=-0.2, vmax=0.2, cmap='PiYG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P4.2 Panorama Stitching\n",
    "\n",
    "In this question, you will be building a panorama from images you generate from Blender. This will involve three steps: (1) image generation, (2) image transform estimation, and (3) stitching.\n",
    "\n",
    "**TASK** Generate images from Blender. To do this, you may using the `simple_pano_env.blend` environment that I have provided you with. By rotating the camera (done by modifying the rotation about its Z-axis). You should set the Focal length of the camera to `40 mm` and sweep the rotation from +40 degrees to -60 degrees; you should rotate the camera in increments such that consecutive images have an overlap of roughly 1/3. You will likely need to generate roughly 5 or 6 images in this range.\n",
    "\n",
    "**PLOTS** Reproject the images using the `reproject_image_to_sphere` function from the previous question and compute the translation transform between each pair of \"consecutive images\" (images next to one another in angle space) using OpenCV. For each pair of matched images \n",
    "\n",
    "To compute the transformation, you may use the same [OpenCV Homography tutorial from the last assignment](https://docs.opencv.org/master/d1/de0/tutorial_py_feature_homography.html). However, we know that the transformation is a translation, and so we do not want to allow the system to generate a general homography matrix, which is what results with `cv.findHomography`. Instead, you should use `affine_mat = cv.estimateAffinePartial2D(src_pts, dst_pts)[0]`, which returns a `2x3` matrix (you will need to convert this to a `3x3` homography by adding a row of `[0, 0, 1]`) that only allows for scale, rotation, and translation. Create a new transformation matrix that includes only the estimated translation parameters. Using this procedure should be more numerically stable.\n",
    "\n",
    "**PLOT** Create the panorama and include it in a plot! To do this you should:\n",
    "\n",
    "1. Pad all images to the size of the output panorama (you will need to determine how wide this will need to be).\n",
    "2. Apply the transformation matrices (using `cv.warpPerspective`) to the images to move them \"into place\" (the location they will be in the resulting panorama). This means that you will need to apply `translation_mat_2_to_1` (or its inverse) to shift image 2 relative to image 1. Note that moving image 3 into place will require accounting for the translation between 2 and 3 *and* the translation between 1 and 2, and so on. You should prefer to multiply the transformation matrices together before using them to transform the image.\n",
    "3. Combine the images to make the panorama. You do not need to use any of the \"fancy\" blending techniques we discussed in class. Simply using `np.maximum` between the two images will create a sufficient panorama. Small artifacts from merging are acceptable.\n",
    "\n",
    "**PLOT** Finally, add the 20 mm focal length image you generated as part of the previous question to your panorama. It might be interesting to see how the significant change in field of view reveals more of the panorama at once and more of the space above and below the horizon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P4.3 Triangulation \n",
    "\n",
    "In class, we discussed how you could extract information about a 3D scene given two cameras and their camera projection matrices. Here, we will investigate a simple example to learn the fundamentals.\n",
    "\n",
    "### P4.3.1 Projecting Into Image Space\n",
    "\n",
    "Below, I have provided you with two images taken by two cameras `a` and `b`. In this question, we will go over some camera basics, namely how to compute the image-space point from a 3D point in the scene and the known camera matrices.\n",
    "\n",
    "Some information about the two camera matrices:\n",
    "- The first camera is translated such that `t_a = [0, -0.2, 5]` and `t_b = [-1.5, 0, 5]`\n",
    "- No rotation is applied to either camera (so the rotation matrix is the identity matrix)\n",
    "- The focal length of the camera (for these 1024 px) images is `f = 1170.3` (in units of pixels).\n",
    "- The camera center is located at the center of the image.\n",
    "\n",
    "**QUESTION** What are the camera matrices $P_a$ and $P_b$? I will accept either the final matrix, or the matrix written in terms of its component matrices (the intrinsic and extrinsic matrices), as long as these are defined.\n",
    "\n",
    "I have provided you with a single point below in 3D space `X0` that exists on one of the corners of the cube shown in the scene.\n",
    "\n",
    "**TASK + PLOTS** Implement the function `get_projected_point(P, X)` which takes in a camera matrix `P` and a 3D scene point `X`. If your matrices are implemented correctly, you should see that the projected 3D point overlaps with one of the corners of the cube in image space. Include the two images with the point `X0` projected onto the two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Starter code\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def load_image(filepath):\n",
    "    \"\"\"Loads an image into a numpy array.\n",
    "    Note: image will have 3 color channels [r, g, b].\"\"\"\n",
    "    img = Image.open(filepath)\n",
    "    img = np.asarray(img).astype(np.float)/255\n",
    "    return img[:, :, :3]\n",
    "\n",
    "image_a = load_image('two_view_cube_image_a.png')\n",
    "image_b = load_image('two_view_cube_image_b.png')\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.imshow(image_a)\n",
    "plt.subplot(122)\n",
    "plt.imshow(image_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Implement the camera matrices & get_projected_point\n",
    "f = 1137.8\n",
    "Pa = None\n",
    "Pb = None\n",
    "\n",
    "X0 = np.array([ 0.85244616, 0.9508618, -0.51819406])\n",
    "points_3D = [X0]\n",
    "\n",
    "def get_projected_point(P, X):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting Code\n",
    "if Pa is None or Pb is None:\n",
    "    raise NotImplementedError(\"Define the camera matrices.\")\n",
    "\n",
    "def visualize_projected_points(image, P, points_3D, verbose=False):\n",
    "    plt.figure(dpi=100)\n",
    "    plt.imshow(image)\n",
    "    for X in points_3D:\n",
    "        x = get_projected_point(P, X)\n",
    "        if verbose:\n",
    "            print(x)\n",
    "        plt.plot(x[0], x[1], 'ko')\n",
    "\n",
    "visualize_projected_points(image_a, Pa, points_3D)\n",
    "visualize_projected_points(image_b, Pb, points_3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P4.3.2 Determining the Size of the Cube\n",
    "\n",
    "Now you will invert this operation. In class, we discussed how to triangulate a point from two correspondences. The relevant slide from L08.1 is as follows:\n",
    "\n",
    "<img src=\"triangulation_lin_alg.png\" width=\"400\">\n",
    "\n",
    "(*Note*: I have used `Pa` and `Pb` to denote the image matrices, whereas the included slide uses $p$ and $p'$.) You can use SVD to solve for the \"best\" value of the 3D point $X$ (equivalently, you can find the minimum eigenvector of $A^T A$). Manually determine the (x, y) coordinates of two corners in the provided images (from the upper left corner) and use them as part of this triangulation procedure. By finding the 3D point corresponding to two of the corners and computing the distance between them, you should be able to compute the size of the cube in the images.\n",
    "\n",
    "**TASK** Pick two corners of the cube and include the $(x, y)$ image coordinates for both `image_a` and `image_b` and the 3D world coordinate $(X, Y, Z)$ in your writeup.\n",
    "\n",
    "**QUESTION** What is the side length of the cube shown in the two images above? (The answer might be somewhat sensitive to the coordinates you measure in image space, though we are only looking for a \"close enough\" number within maybe 10% of the \"correct\" answer. You should feel free to use more than two points and average the results to get a more accurate result.)\n",
    "\n",
    "You can confirm that your estimated 3D coordinates are correct by reprojecting them back into image space using your solution from the previous question to check for accuracy.\n",
    "\n",
    "*We will use your full response to evaluate partial credit, so be sure to enumerate the steps you took and (if you feel it helpful) intermediate results or code snippets.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P4.4 Stereo Patch Matching\n",
    "\n",
    "Now I have provided you with a stereo pair of images (already rectified) and a handful of features in one of the images. Your job is to locate the locations of the corresponding features in the other image using *patch match stereo* as we discussed in class. I have provided you with some starter code in the function `patch_match_stereo` below, which iterates through the possible locations\n",
    "\n",
    "**QUESTION** The possible feature matches in the second image are along the epipolar line. Since the images are properly rectified, what is the epipolar line in the second image corresponding to coordinate `(x_a, y_a)` in the first image?\n",
    "\n",
    "**TASK** Define the `possible_coordinates` vector in the `patch_match_stereo` function using your answer. Once that is defined, the `patch_match_stereo` function will loop through all possible feature coordinates in the second image and return the coordinate with the best *match_score*.\n",
    "\n",
    "**TASK** Implement the function `compute_match_score_ssd` (Sum of Squared Differences) using the formula we discussed in class: $$ \\text{response} = -\\sum_{k,l} (g_{kl} - f_{kl})^2, $$ where $g$ is the patch from `image_a` and $f$ is the patch from `image_b`. If this function is correctly implemented, you should see some of the features are aligned between the two images.\n",
    "\n",
    "**TASK** Implement the function `compute_match_score_ncc` (Normalized Cross Correlation) using the formula: $$ \\text{response} = \\frac{\\sum_{k,l}(g_{kl} - \\bar{g})(f_{kl} - \\bar{f})}{\\sqrt{\\sum_{kl}(g_{kl} - \\bar{g})^2}\\sqrt{\\sum_{kl}(f_{kl} - \\bar{f})^2}}$$\n",
    "\n",
    "Once you have implemented these functions, you should run the plotting code I have included below, which computes a disparity map over the entire image. **NOTE: this will take a long time to run, so be sure that you confirm that your code is working properly first. You may want to test using the code from the breakout session L08B first.**\n",
    "\n",
    "**PLOTS** Include in your writeup the depth plots generated by each of the two match scores generated by the code below in the code block beginning with `# Compute and plot the depth maps`.\n",
    "\n",
    "**QUESTION** The left few columns of both depth maps is quite noisy and inaccurate. Give an explanation for why this is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from https://vision.middlebury.edu/stereo/data/scenes2005/\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import re\n",
    "import scipy.signal\n",
    "import cv2\n",
    "\n",
    "\n",
    "# Load the images\n",
    "def load_image_gray(filepath):\n",
    "    \"\"\"Loads an image into a numpy array.\n",
    "    Note: image will have 3 color channels [r, g, b].\"\"\"\n",
    "    img = Image.open(filepath)\n",
    "    img = np.asarray(img).astype(np.float)/255\n",
    "    if len(img.shape) > 2:\n",
    "        return img[:, :, 0]\n",
    "    else:\n",
    "        return img\n",
    "\n",
    "image_a = load_image_gray('art_view0.png')\n",
    "image_b = load_image_gray('art_view5.png')\n",
    "\n",
    "## Plotting Code\n",
    "plt.figure(figsize=(12, 5))\n",
    "ax_a = plt.subplot(1, 2, 1)\n",
    "plt.imshow(image_a, cmap='gray')\n",
    "ax_b = plt.subplot(1, 2, 2)\n",
    "plt.imshow(image_b, cmap='gray')\n",
    "\n",
    "\n",
    "def get_patch(image, x, y, patch_half_width):\n",
    "    return image[y-patch_half_width:y+patch_half_width+1,\n",
    "                 x-patch_half_width:x+patch_half_width+1]\n",
    "\n",
    "def compute_match_score_ssd(patch_a, patch_b):\n",
    "    raise NotImplementedError(\"Define the patch match score\")\n",
    "    \n",
    "def compute_match_score_ncc(patch_a, patch_b):\n",
    "    raise NotImplementedError(\"Define the patch match score\")\n",
    "\n",
    "def patch_match_stereo(image_a, image_b, x_a, y_a,\n",
    "                       match_score_fn,\n",
    "                       patch_half_width=9):\n",
    "    \"\"\"Returns the location of a feature/patch between stereo images.\n",
    "    Inputs are the x, y coordinates of the patch in image_a.\n",
    "    Outputs are the x, y coordinates of the patch in image_b.\"\"\"\n",
    "\n",
    "    # (1) Get the patch in image a\n",
    "    patch_a = get_patch(image_a, x_a, y_a, \n",
    "                        patch_half_width=patch_half_width)\n",
    "\n",
    "    # (2) Compute the responses along the epipolar line in image b\n",
    "    # Define the possible coordinates along with the match might be found\n",
    "    # (You should feel free to modify this code if you have a simpler\n",
    "    # way to represent this operation.)\n",
    "    possible_coordinates = None\n",
    "    if possible_coordinates is None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    response = np.zeros((len(possible_coordinates)))\n",
    "    for ind, (x_b, y_b) in enumerate(possible_coordinates):\n",
    "        # Get the patch\n",
    "        patch_b = get_patch(image_b, x_b, y_b,\n",
    "                            patch_half_width=patch_half_width)\n",
    "        # Compute the match score & store\n",
    "        response[ind] = match_score_fn(patch_a, patch_b)\n",
    "    \n",
    "    # (3) Compute the maximum response\n",
    "    ind = np.argmax(response)\n",
    "    x_b, y_b = possible_coordinates[ind]\n",
    "\n",
    "    return x_b, y_b, responses[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute and plot the depth maps\n",
    "\n",
    "phw = 15\n",
    "spacing = 10\n",
    "\n",
    "def compute_depth_map(image_a, image_b, match_score_fn):\n",
    "    xs = range(phw, image_b.shape[1]-phw, spacing)\n",
    "    ys = range(phw, image_b.shape[0]-phw, spacing)\n",
    "    disparity_mat = np.zeros((len(ys), len(xs)))\n",
    "    responses_mat = np.zeros((len(ys), len(xs)))\n",
    "    for xi, x_a in enumerate(xs):\n",
    "        print(f\"Progress: {xi}/{len(xs)}\")\n",
    "        for yi, y_a in enumerate(ys):\n",
    "            x_b, y_b, response = patch_match_stereo(\n",
    "                image_a, image_b, x_a, y_a, match_score_fn, patch_half_width=phw)\n",
    "            dx = x_a - x_b\n",
    "            dy = y_a - y_b\n",
    "            disparity_mat[yi, xi] = np.sqrt(dx**2 + dy**2)\n",
    "            responses_mat[yi, xi] = response\n",
    "\n",
    "    # Compute and threshold the depth map\n",
    "    depth = 1/(disparity_mat.copy() + 1e-5)\n",
    "    depth[depth > 0.01] = 0.01\n",
    "    return depth\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(compute_depth_map(image_a, image_b, compute_match_score_ssd))\n",
    "plt.title('Depth Map (SSD)')\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(compute_depth_map(image_a, image_b, compute_match_score_ncc))\n",
    "plt.title('Depth Map (NCC)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (CS682)",
   "language": "python",
   "name": "cs682venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
